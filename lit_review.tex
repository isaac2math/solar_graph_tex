\subsection{Literature review on graphical causal inference, graph learning and variable selection}

\subsubsection*{Variable selection and corresponding issues in linear graph learning}

In empirical researches, a graph learning task typically starts from linear graph learning (e.g., \citet{bollen1989structural, geiger1994learning, spirtes2000causation, friedman2008sparse}). Compared with nonlinear graphs on the same dataset, linear graphs requires much less computation load and is easier for inference. Also, linear graph is deeply related to linear modelling methods like linear regression, best subset variable selection, shrinkage and lasso. Only if linearity cannot approximate the dependency in the data, researchers apply nonlinear dependence measure like Hilbert Schmidt independence criterion \citep{gretton2005measuring} and mutual information.

As a major issue in linear graph learning, multicollinearity is frequently observed among variables with complicated causal structures, which will cause several problems for the parameter estimation in both linear modelling and linear graph learning. First, since linear model estimation is based on error minimization, multicollinearity will reduce the magnitude of the minimal eigenvalue in the linear space, causing numerical convergence problems (e.g., Cholesky decomposition or gradient descent) when applying maximum liklihood or maximum a posteriori. Second, severe multicollinearity amplifies parameter estimate instability across samples, making it difficult to interpret the coefficients reliably and accurately. Third, multicollinearity causes problems for statistical tests that rely on the sample covariance (e.g., the post-OLS t-test or the lasso covariance test \citep{lockhart2014significance}. The conditional correlation tests of in constraint-based learning \citep{farrar1967multicollinearity}). Last but not least, multicollinearity may also reduce the algorithmic stability of the model \citep{elisseeff2003leave}, which reduces the generalization ability and the out-of-sample prediction of the estimated model.

Moreover, multicollinearity also affects the reliability of variable selection algorithms in linear modelling and linear graph learning. \citet{zou2005regularization, jia2010model} find that, if a group of variables are highly correlated with one another, lasso-type estimators may randomly select one variable from the group and drop the others out of the regression, referred to as \textbf{the grouping effect}. Since all linear modelling techniques make the variable selection decision based on the conditional correlation between $\mathbf{x}_j$ and $\mathbf{y}$, the grouping effect may well apply to all variable selection methods in linear models. Multicollinearity also affect linear graph learning. \citet{heckerman95, chickering04} shows that learning a linear graph is NP-hard on data with large $p$. As a result, graph learning algorithms typically work well on data with large $n$ and very sparse $p$. In many graph learning applications, variable selection algorithms (e.g., SCAD \citep{fan2001variable}, ISIS \citep{fan2008sure} or different lasso-type estimators \citep{fan2009network}) are used to filter out redundant variables before graph learning. As a result, with grouping effect, variable selection methods may randomly drop some of the highly correlated variables, resulting in omissions of important variables in the linear graph learning.

Many attempts have been made to reduce the effects of multicollinearity. For more stable regression coefficient estimates, \citet{hoerlkennard70} apply Tikhonov regularization to OLS, resulting in the ridge regression. However, it complicates statistical tests and post-estimation inference. To reduce the grouping effect and obtain stable variable-selection results, cross-validated group lasso and cross-validated elastic net (CV-en) have been introduced \citep{zou2005regularization, friedman10}. However, group lasso relies on manual grouping of variables, which depends heavily on accurate field knowledge. On the other hand, while \citet{zou2005regularization} and \citet{jia2010model} show that CV-en may improve the stability of variable selection, \citet{jia2010model} counter that the improvement is marginal and that ``when the lasso does not select the true model, it is more likely that the elastic net does not select the true model either.''